# Alternative Solutions for Hardcoded Path Issues

## Issue 1: Hardcoded Workspace Paths

**Current Problem**:
```python
# pipelines/transform/bronze_logistics.py:9
sys.path.insert(0, '/Workspace/Users/kaustav.paul@databricks.com/ace-demo/pipelines')
```

---

### **Alternative 1: Environment Variables (Recommended for Shared Demos)**

**Approach**: Use DLT pipeline configuration + environment variables

**Implementation**:
```python
# pipelines/transform/bronze_logistics.py
import dlt
import sys
import os

# Get workspace path from DLT configuration or environment
WORKSPACE_PATH = spark.conf.get("WORKSPACE_PATH", os.getenv("WORKSPACE_PATH"))
sys.path.insert(0, f'{WORKSPACE_PATH}/pipelines')

from config.config import LOGISTICS_SCHEMA, TELEMETRY_PATH, TELEMETRY_CHECKPOINT
```

**Pipeline Config**:
```json
{
  "configuration": {
    "WORKSPACE_PATH": "/Workspace/Users/${current_user}/ace-demo"
  }
}
```

**Pros**:
- ‚úÖ Works across different users automatically
- ‚úÖ Easy to override for different environments
- ‚úÖ Standard Databricks pattern
- ‚úÖ No code changes needed per user

**Cons**:
- ‚ö†Ô∏è Requires updating pipeline_config.json
- ‚ö†Ô∏è Users must understand DLT configuration

**Best For**: Production deployments, shared demos, multi-user environments

---

### **Alternative 2: Dynamic Path Resolution (Zero Configuration)**

**Approach**: Calculate workspace path dynamically using `dbutils`

**Implementation**:
```python
# pipelines/transform/bronze_logistics.py
import dlt
import sys

# Dynamically determine workspace path from notebook context
def get_workspace_root():
    """Auto-detect workspace path from current notebook location"""
    try:
        # Get current notebook path
        notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()
        # Extract workspace root (everything before /pipelines/)
        if '/pipelines/' in notebook_path:
            return notebook_path.split('/pipelines/')[0]
        else:
            # Fallback: assume standard structure
            return '/'.join(notebook_path.split('/')[:-2])
    except:
        # Fallback for local development or testing
        return '/Workspace/Users/kaustav.paul@databricks.com/ace-demo'

WORKSPACE_ROOT = get_workspace_root()
sys.path.insert(0, f'{WORKSPACE_ROOT}/pipelines')

from config.config import LOGISTICS_SCHEMA, TELEMETRY_PATH, TELEMETRY_CHECKPOINT
```

**Pros**:
- ‚úÖ Zero configuration required
- ‚úÖ Works for any user automatically
- ‚úÖ Self-contained solution
- ‚úÖ No pipeline config changes needed

**Cons**:
- ‚ö†Ô∏è Relies on `dbutils` (not available in local testing)
- ‚ö†Ô∏è Slightly more complex logic
- ‚ö†Ô∏è Harder to override for custom deployments

**Best For**: Quick demos, user-shared notebooks, minimal setup scenarios

---

### **Alternative 3: Relative Imports (Cleanest Approach)**

**Approach**: Restructure code to avoid `sys.path` manipulation entirely

**Implementation**:

**Option 3A: Use Databricks-style relative imports**
```python
# pipelines/transform/bronze_logistics.py
import dlt
from pipelines.config.config import LOGISTICS_SCHEMA, TELEMETRY_PATH, TELEMETRY_CHECKPOINT

# Rest of code...
```

**Pipeline Config** (key addition):
```json
{
  "configuration": {
    "spark.databricks.python.path": "/Workspace/Users/${current_user}/ace-demo"
  }
}
```

**Option 3B: Package-based approach**
```bash
# Restructure as a proper Python package
ace-hardware-demo/
‚îú‚îÄ‚îÄ setup.py  # New file
‚îú‚îÄ‚îÄ ace_demo/  # Rename pipelines/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ transform/
‚îÇ   ‚îî‚îÄ‚îÄ analytics/
```

```python
# setup.py
from setuptools import setup, find_packages

setup(
    name="ace-demo",
    version="1.0.0",
    packages=find_packages(),
    install_requires=['pyspark>=3.5.0']
)
```

```python
# pipelines/transform/bronze_logistics.py
import dlt
from ace_demo.config.config import LOGISTICS_SCHEMA, TELEMETRY_PATH, TELEMETRY_CHECKPOINT
```

**Pros**:
- ‚úÖ No `sys.path` manipulation (cleaner)
- ‚úÖ Standard Python packaging
- ‚úÖ Works in local testing and Databricks
- ‚úÖ Better for version control and distribution

**Cons**:
- ‚ö†Ô∏è Requires restructuring project
- ‚ö†Ô∏è More setup overhead (pip install, wheel building)
- ‚ö†Ô∏è Overkill for simple demos

**Best For**: Production codebases, reusable packages, complex projects

---

### **Alternative 4: Symbolic Constants File**

**Approach**: Create a separate paths configuration file

**Implementation**:
```python
# pipelines/workspace_config.py (NEW FILE)
import os

# Detect current user from environment or use default
CURRENT_USER = os.getenv("DATABRICKS_USER", "kaustav.paul@databricks.com")
WORKSPACE_ROOT = f"/Workspace/Users/{CURRENT_USER}/ace-demo"

# Derived paths
PIPELINES_ROOT = f"{WORKSPACE_ROOT}/pipelines"
```

```python
# pipelines/transform/bronze_logistics.py
import dlt
import sys
from pipelines.workspace_config import PIPELINES_ROOT

sys.path.insert(0, PIPELINES_ROOT)
from config.config import LOGISTICS_SCHEMA, TELEMETRY_PATH, TELEMETRY_CHECKPOINT
```

**Pros**:
- ‚úÖ Centralized path management
- ‚úÖ Easy to find and update
- ‚úÖ Single file to modify per deployment
- ‚úÖ Can include other workspace-specific configs

**Cons**:
- ‚ö†Ô∏è Still requires one file to be edited
- ‚ö†Ô∏è `DATABRICKS_USER` env var might not be set
- ‚ö†Ô∏è Adds extra import

**Best For**: Teams with standardized workspace structure, mid-size projects

---

### **Alternative 5: Workspace Init Script**

**Approach**: Use Databricks init scripts to set PYTHONPATH globally

**Implementation**:

**Create init script**:
```bash
# /Workspace/Shared/init-scripts/ace-demo-pythonpath.sh
#!/bin/bash

# Set PYTHONPATH for all notebooks in workspace
export PYTHONPATH="$PYTHONPATH:/Workspace/Users/${DB_USER}/ace-demo/pipelines"
```

**Configure cluster/pipeline** to use this init script

**Pipeline code** (no sys.path needed):
```python
# pipelines/transform/bronze_logistics.py
import dlt
from config.config import LOGISTICS_SCHEMA, TELEMETRY_PATH, TELEMETRY_CHECKPOINT
# Works because PYTHONPATH is set globally
```

**Pros**:
- ‚úÖ Code is cleanest (no path manipulation)
- ‚úÖ Applies to all notebooks on cluster
- ‚úÖ One-time cluster setup

**Cons**:
- ‚ö†Ô∏è Requires cluster admin access
- ‚ö†Ô∏è Hidden configuration (not obvious from code)
- ‚ö†Ô∏è Harder to debug if misconfigured

**Best For**: Organization-wide deployments, enterprise environments

---

### **Comparison Table: Workspace Path Solutions**

| Solution | Setup Complexity | Code Cleanliness | Portability | Best Use Case |
|----------|------------------|------------------|-------------|---------------|
| **Alt 1: Env Variables** | üü° Medium | üü¢ Good | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **Recommended Default** |
| **Alt 2: Dynamic Resolution** | üü¢ Low | üü° Medium | ‚≠ê‚≠ê‚≠ê‚≠ê | Quick demos |
| **Alt 3: Relative Imports** | üî¥ High | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Production apps |
| **Alt 4: Symbolic Constants** | üü¢ Low | üü° Medium | ‚≠ê‚≠ê‚≠ê | Team projects |
| **Alt 5: Init Scripts** | üî¥ High | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | Enterprise |

---

## Issue 2: Pipeline Config Uses Old Paths

**Current Problem**:
```json
{
  "libraries": [
    {
      "notebook": {
        "path": "/Workspace/Users/.../pipelines/bronze_logistics.py"
      }
    }
  ]
}
```

**Should reference**: `/pipelines/transform/bronze_logistics.py`

---

### **Alternative 1: Manual Update (Simple Fix)**

**Approach**: Directly edit `pipeline_config.json`

**Implementation**:
```json
{
  "name": "ace_logistics_pipeline",
  "catalog": "kaustavpaul_demo",
  "target": "ace_demo",
  "libraries": [
    {
      "notebook": {
        "path": "/Workspace/Users/kaustav.paul@databricks.com/ace-demo/pipelines/transform/bronze_logistics.py"
      }
    },
    {
      "notebook": {
        "path": "/Workspace/Users/kaustav.paul@databricks.com/ace-demo/pipelines/transform/bronze_dimensions.py"
      }
    },
    {
      "notebook": {
        "path": "/Workspace/Users/kaustav.paul@databricks.com/ace-demo/pipelines/transform/silver_logistics.py"
      }
    },
    {
      "notebook": {
        "path": "/Workspace/Users/kaustav.paul@databricks.com/ace-demo/pipelines/transform/gold_flo_metrics.py"
      }
    },
    {
      "notebook": {
        "path": "/Workspace/Users/kaustav.paul@databricks.com/ace-demo/pipelines/analytics/analytics_views.sql"
      }
    }
  ]
}
```

**Pros**:
- ‚úÖ Immediate fix (5 minutes)
- ‚úÖ No code changes
- ‚úÖ Clear and explicit

**Cons**:
- ‚ö†Ô∏è Still has hardcoded username
- ‚ö†Ô∏è Manual process
- ‚ö†Ô∏è Easy to forget when adding new notebooks

**Best For**: Quick fix while planning longer-term solution

---

### **Alternative 2: Templated Config with Variables**

**Approach**: Use environment variables in config file

**Implementation**:

**Create**: `pipeline_config.template.json`
```json
{
  "name": "ace_logistics_pipeline",
  "catalog": "${CATALOG}",
  "target": "${SCHEMA}",
  "libraries": [
    {
      "notebook": {
        "path": "${WORKSPACE_PATH}/pipelines/transform/bronze_logistics.py"
      }
    },
    {
      "notebook": {
        "path": "${WORKSPACE_PATH}/pipelines/transform/bronze_dimensions.py"
      }
    },
    {
      "notebook": {
        "path": "${WORKSPACE_PATH}/pipelines/transform/silver_logistics.py"
      }
    },
    {
      "notebook": {
        "path": "${WORKSPACE_PATH}/pipelines/transform/gold_flo_metrics.py"
      }
    },
    {
      "notebook": {
        "path": "${WORKSPACE_PATH}/pipelines/analytics/analytics_views.sql"
      }
    }
  ],
  "configuration": {
    "WORKSPACE_PATH": "${WORKSPACE_PATH}"
  }
}
```

**Create setup script**: `scripts/setup_pipeline_config.sh`
```bash
#!/bin/bash

# Get user input or use defaults
CATALOG=${CATALOG:-"kaustavpaul_demo"}
SCHEMA=${SCHEMA:-"ace_demo"}
WORKSPACE_USER=${DATABRICKS_USER:-"kaustav.paul@databricks.com"}
WORKSPACE_PATH="/Workspace/Users/${WORKSPACE_USER}/ace-demo"

# Generate pipeline_config.json from template
sed -e "s|\${CATALOG}|${CATALOG}|g" \
    -e "s|\${SCHEMA}|${SCHEMA}|g" \
    -e "s|\${WORKSPACE_PATH}|${WORKSPACE_PATH}|g" \
    pipeline_config.template.json > pipeline_config.json

echo "‚úÖ Generated pipeline_config.json for user: ${WORKSPACE_USER}"
```

**Usage**:
```bash
# User runs once during setup
./scripts/setup_pipeline_config.sh
```

**Pros**:
- ‚úÖ Template is portable
- ‚úÖ Easy to regenerate for different users
- ‚úÖ Keeps original as template
- ‚úÖ Can version-control template

**Cons**:
- ‚ö†Ô∏è Requires running setup script
- ‚ö†Ô∏è Generated file must be in .gitignore
- ‚ö†Ô∏è Extra step for users

**Best For**: Multi-user demos, different environment deployments

---

### **Alternative 3: Use Databricks CLI to Create Pipeline**

**Approach**: Don't use JSON file at all - create pipeline via CLI

**Implementation**:

**Create**: `scripts/create_pipeline.sh`
```bash
#!/bin/bash

# Load configuration
source .env

CATALOG="${CATALOG:-kaustavpaul_demo}"
SCHEMA="${SCHEMA:-ace_demo}"
WORKSPACE_PATH="${WORKSPACE_PATH:-/Workspace/Users/${DATABRICKS_USER}/ace-demo}"

# Create DLT pipeline using Databricks CLI
databricks pipelines create \
  --name "ace_logistics_pipeline" \
  --catalog "$CATALOG" \
  --target "$SCHEMA" \
  --notebook "$WORKSPACE_PATH/pipelines/transform/bronze_logistics.py" \
  --notebook "$WORKSPACE_PATH/pipelines/transform/bronze_dimensions.py" \
  --notebook "$WORKSPACE_PATH/pipelines/transform/silver_logistics.py" \
  --notebook "$WORKSPACE_PATH/pipelines/transform/gold_flo_metrics.py" \
  --notebook "$WORKSPACE_PATH/pipelines/analytics/analytics_views.sql" \
  --continuous false \
  --channel "PREVIEW" \
  --configuration "WORKSPACE_PATH=$WORKSPACE_PATH" \
  > pipeline_id.txt

echo "‚úÖ Pipeline created with ID: $(cat pipeline_id.txt)"
echo "View at: ${DATABRICKS_HOST}/pipelines/$(cat pipeline_id.txt)"
```

**Pros**:
- ‚úÖ No JSON file needed
- ‚úÖ Fully programmatic
- ‚úÖ Easy to automate
- ‚úÖ Can include in setup automation

**Cons**:
- ‚ö†Ô∏è Requires Databricks CLI installed
- ‚ö†Ô∏è No visual config file to reference
- ‚ö†Ô∏è Pipeline updates require CLI commands

**Best For**: CI/CD pipelines, automated deployments, infrastructure as code

---

### **Alternative 4: Databricks Asset Bundles (DABs)**

**Approach**: Use modern Databricks Asset Bundle format

**Implementation**:

**Create**: `databricks.yml`
```yaml
bundle:
  name: ace-hardware-demo

workspace:
  host: ${DATABRICKS_HOST}

variables:
  catalog:
    default: kaustavpaul_demo
    description: Unity Catalog name
  
  schema:
    default: ace_demo
    description: Schema name
  
  workspace_path:
    default: /Workspace/Users/${workspace.current_user.userName}/ace-demo
    description: Workspace root path

resources:
  pipelines:
    ace_logistics_pipeline:
      name: ace_logistics_pipeline
      catalog: ${var.catalog}
      target: ${var.schema}
      
      libraries:
        - notebook:
            path: ${var.workspace_path}/pipelines/transform/bronze_logistics.py
        - notebook:
            path: ${var.workspace_path}/pipelines/transform/bronze_dimensions.py
        - notebook:
            path: ${var.workspace_path}/pipelines/transform/silver_logistics.py
        - notebook:
            path: ${var.workspace_path}/pipelines/transform/gold_flo_metrics.py
        - notebook:
            path: ${var.workspace_path}/pipelines/analytics/analytics_views.sql
      
      clusters:
        - label: default
          autoscale:
            min_workers: 1
            max_workers: 5
            mode: ENHANCED
      
      configuration:
        WORKSPACE_PATH: ${var.workspace_path}
      
      continuous: false
      channel: PREVIEW
      photon: false

targets:
  dev:
    mode: development
    workspace:
      host: https://e2-demo-field-eng.cloud.databricks.com
  
  prod:
    mode: production
    workspace:
      host: ${DATABRICKS_HOST}
```

**Deployment**:
```bash
# Validate bundle
databricks bundle validate

# Deploy to dev
databricks bundle deploy -t dev

# Deploy to prod
databricks bundle deploy -t prod
```

**Pros**:
- ‚úÖ Modern Databricks standard
- ‚úÖ Automatic user resolution (`${workspace.current_user.userName}`)
- ‚úÖ Environment management (dev/prod)
- ‚úÖ Full workspace deployment (notebooks, jobs, pipelines)
- ‚úÖ Validation before deployment

**Cons**:
- ‚ö†Ô∏è Requires learning new format
- ‚ö†Ô∏è More complex initial setup
- ‚ö†Ô∏è Requires Databricks CLI v0.200+

**Best For**: Modern Databricks projects, production deployments, enterprise

---

### **Alternative 5: Python-based Configuration**

**Approach**: Generate config from Python script

**Implementation**:

**Create**: `scripts/generate_pipeline_config.py`
```python
#!/usr/bin/env python3
import json
import os
from pathlib import Path

def generate_pipeline_config(
    catalog: str = "kaustavpaul_demo",
    schema: str = "ace_demo",
    workspace_user: str = None,
):
    """Generate pipeline_config.json for current user/environment"""
    
    # Auto-detect user if not provided
    if workspace_user is None:
        workspace_user = os.getenv("DATABRICKS_USER", "kaustav.paul@databricks.com")
    
    workspace_path = f"/Workspace/Users/{workspace_user}/ace-demo"
    
    # Define pipeline structure
    notebooks = [
        "pipelines/transform/bronze_logistics.py",
        "pipelines/transform/bronze_dimensions.py",
        "pipelines/transform/silver_logistics.py",
        "pipelines/transform/gold_flo_metrics.py",
        "pipelines/analytics/analytics_views.sql",
    ]
    
    config = {
        "name": "ace_logistics_pipeline",
        "catalog": catalog,
        "target": schema,
        "clusters": [
            {
                "label": "default",
                "autoscale": {
                    "min_workers": 1,
                    "max_workers": 5,
                    "mode": "ENHANCED"
                }
            }
        ],
        "libraries": [
            {"notebook": {"path": f"{workspace_path}/{nb}"}}
            for nb in notebooks
        ],
        "configuration": {
            "WORKSPACE_PATH": workspace_path,
            "TELEMETRY_PATH": f"/Volumes/{catalog}/{schema}/ace_files/data/telemetry/",
            "DIMENSIONS_PATH": f"/Volumes/{catalog}/{schema}/ace_files/data/dimensions",
        },
        "channel": "PREVIEW",
        "photon": False,
        "continuous": False
    }
    
    # Write to file
    output_path = Path(__file__).parent.parent / "pipeline_config.json"
    with open(output_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"‚úÖ Generated {output_path}")
    print(f"   User: {workspace_user}")
    print(f"   Catalog: {catalog}")
    print(f"   Schema: {schema}")
    
    return config

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Generate DLT pipeline config")
    parser.add_argument("--catalog", default="kaustavpaul_demo")
    parser.add_argument("--schema", default="ace_demo")
    parser.add_argument("--user", help="Databricks username (email)")
    
    args = parser.parse_args()
    generate_pipeline_config(args.catalog, args.schema, args.user)
```

**Usage**:
```bash
# Generate for current user
python scripts/generate_pipeline_config.py

# Generate for specific user
python scripts/generate_pipeline_config.py --user john.doe@company.com

# Generate for different catalog/schema
python scripts/generate_pipeline_config.py --catalog my_catalog --schema my_schema
```

**Pros**:
- ‚úÖ Type-safe configuration
- ‚úÖ Easy to add validation logic
- ‚úÖ Can auto-detect environment
- ‚úÖ Programmatic and extensible
- ‚úÖ No external dependencies

**Cons**:
- ‚ö†Ô∏è Requires running Python script
- ‚ö†Ô∏è One more tool in the chain

**Best For**: Python-heavy teams, custom validation needs, complex configs

---

### **Comparison Table: Pipeline Config Solutions**

| Solution | Setup Time | Maintenance | Automation | Best Use Case |
|----------|-----------|-------------|------------|---------------|
| **Alt 1: Manual Update** | 5 min | Low | ‚ùå None | Quick fix |
| **Alt 2: Template + Script** | 15 min | Medium | üü° Bash | **Recommended** |
| **Alt 3: Databricks CLI** | 10 min | Low | ‚úÖ Full | CI/CD pipelines |
| **Alt 4: Asset Bundles** | 30 min | Low | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **Modern standard** |
| **Alt 5: Python Generator** | 20 min | Medium | ‚úÖ Full | Custom validation |

---

## Recommended Combination

For **maximum portability and ease of use**, I recommend:

### For Workspace Paths:
**Use Alternative 1 (Environment Variables)** + **Alternative 2 (Dynamic Resolution) as fallback**

```python
# pipelines/transform/bronze_logistics.py
import dlt
import sys
import os

def get_workspace_path():
    """Get workspace path with multiple fallback options"""
    # Option 1: From DLT pipeline configuration
    try:
        workspace_path = spark.conf.get("WORKSPACE_PATH")
        if workspace_path:
            return workspace_path
    except:
        pass
    
    # Option 2: From environment variable
    workspace_path = os.getenv("WORKSPACE_PATH")
    if workspace_path:
        return workspace_path
    
    # Option 3: Auto-detect from notebook context
    try:
        notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()
        if '/pipelines/' in notebook_path:
            return notebook_path.split('/pipelines/')[0]
    except:
        pass
    
    # Option 4: Fallback to default (for local testing)
    return '/Workspace/Users/kaustav.paul@databricks.com/ace-demo'

WORKSPACE_PATH = get_workspace_path()
sys.path.insert(0, f'{WORKSPACE_PATH}/pipelines')

from config.config import LOGISTICS_SCHEMA, TELEMETRY_PATH, TELEMETRY_CHECKPOINT
```

### For Pipeline Config:
**Use Alternative 2 (Template + Script)** for now, plan migration to **Alternative 4 (DABs)** for future

This gives you:
- ‚úÖ Works out-of-box for most users (dynamic detection)
- ‚úÖ Easy to override when needed (env vars)
- ‚úÖ Portable config generation (template)
- ‚úÖ Future-ready for modern Databricks (DABs migration path)

---

## Implementation Priority

1. **Immediate** (5 min): Manual update pipeline_config.json paths
2. **Phase 1** (30 min): Add dynamic workspace path detection
3. **Phase 2** (1 hour): Create config template + generation script
4. **Phase 3** (Future): Migrate to Databricks Asset Bundles

---

Would you like me to implement any of these alternatives?
